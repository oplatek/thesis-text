\chapter{Overview of used techniques and algorithms}
\section{Audio signal processing}
\label{ASP-desc}
We sketch here the basic principles as described in \cite{taylor2009text}, so we can understand our input data.
Speech signal in the real world are mechanical waves, so it is obviously analogous quantity.
When we process the speech signal with computers, it is assumed to be digitised, so it is converted into discrete form.
When performing digital signal processing, we are usually concerned with three key issues.
\begin{enumerate}
\item To remove the influence of phase, because the ear is not sensitive to phase information in speech, and we can use frequency domain representation.
\item Performing source/filter separation, so that we can study the \textit{spectral envelope} of sounds.
Spectral envelope characterizes the frequency spectrum of the signal, which is essential for the speech recognition and processing.
\item We often wish to transform these spectral envelopes and source signals into more efficient representations.
\end{enumerate}
Overview of the main standard processing steps follows.
\linebreak\linebreak
The input signal is divided into \textit{windows}.
Windowing considers only some part of the signal.
Usually, the signal is transformed at window borders to prevent discontinuities.
This is achieved for example by use of \textit{Hamming} window.
The complete waveform is therefore a series of windows, that are sometimes called \textit{frames}.
The frames overlap, this is influenced by the size of the \textit{frame shift}.
Inside one frame, we assume, that the speech signal is stationary and we transform it to a frequency domain by Discrete Fourier Transformation\footnote{https://en.wikipedia.org/wiki/Discrete\_Fourier\_transform}.
\linebreak\linebreak
It is better for observing the natural speech characteristics, to use a logarithm scale instead of linear.
In fact, the \textit{mel scale}\footnote{https://en.wikipedia.org/wiki/Mel\_scale} is frequently used, which even better corresponds with the human perception.
Further, the so called \textit{cepstrum}\footnote{https://en.wikipedia.org/wiki/Cepstrum} is computed from the log magnitude spectrum, by performing inverse Fourier transformation.
Cepstrum can be represented by coefficients, number of which can be chosen.
Those are called the \textit{Mel Frequency Cepstral Coefficients(MFCCs)}.
To sum up, we have described how to process the digital audio signal and convert it into discrete series of overlapping frames, each of which is represented by a fixed-length vector of MFCCs.
\linebreak\linebreak
We gave a brief overview of the signal processing procedure that is essential to work with the audio signal.
When we mention the input speech signal, we mean series of MFCC vectors, unless explicitly stated otherwise.
\section{Mel Cepstral distortion}
\label{MCD-desc}
Mel Cepstral Distortion \cite{kubichek1993mel} is a well described measure, that should mirror differences between speech samples. There are some caveats worth stating at the outset.
Firs, there are many other factors that contribute to the perception of voice quality.
For example it takes no account of speech dynamics, either short-range differentials or long-range prosodic
effects.
Moreover, distortions in the pitch contour are ignored.
However, it should reflect the differences in speech quality and pronunciation.
\linebreak\linebreak
As we have described in the previous section, the audio sample can be described by its $cepstrum$.
This cepstrum may be then represented by Mel Frequency Cepstral Coefficients (MFCCs). Its order can be chosen arbitrarily, we used 35 coefficients length vectors. Thus we can transform each audio record into a sequence of float vectors with length 35. The MCD \cite{kubichek1993mel} of two sequences is basically normalized Mean Squared Error between those sequences after aligning. We define it as follows:
\begin{equation}
MCD(v^1, v^2) = \frac{\alpha}{T'}\sum_{ph(t) \neq SIL}\sqrt{\sum_{d=1}^{D}(v_d^1(t), v_d^2(t))^2}
\end{equation}
The $T'$ stands for the number of frames of the shorter of the recordings.
The common use of MCD involves omitting the $0$-th coefficient, since it corresponds to the intensity of signal and therefore it is not usually desired. The scaling factor $\alpha$ is present for historical reasons.
\linebreak\linebreak
The MCD measure has one substantial disadvantage: it does not reflect, that the two compared recordings may not have the same length and, what is more, they can be misaligned.
Thus, we use a modified algorithm that first aligns the sequences using Dynamic Time Warping and then computes MCD with the result.
\section{Finite state transducers}
\label{fst-desc}
Finite state transducers (FSTs) \cite{mohri1997finite}, are basically finite state automata that are augmented by addition of output labels to each transition.
We can further modify the FST and add a weight to each edge.
We define a semiring $(S,\oplus,\otimes,\overline{0},\overline{1})$ as a system that fullfills the following conditions:
\begin{nobreak}
\begin{itemize}
\item $(S,\oplus,\overline{0})$ is a commutative monoid with identity element $\overline{0}$
\item $(S,\otimes,\overline{1})$ is a monoid with identity element $\overline{0}$
\item $\otimes $ distributes over $\oplus$
\item $\forall a \in S: a \otimes \overline{0} = \overline{0} \otimes a = \overline{0}$
\end{itemize}
\end{nobreak}

With this definition of the semiring, we can describe a weighted FST \cite{mohri2009weighted} $T$ over a semiring $S$ formally as a 8-tuple $(\Sigma,\Delta,Q,I,F,E,\lambda,\rho)$ provided that:
\begin{nobreak}
\begin{itemize}
\item $\Sigma, \Delta$ are finite input and output alphabets
\item $Q$ is a finite set of states
\item $I \subset Q$ is a set of initial states
\item $F \subset Q$ is a set of final states
\item $E$ is a multiset of transitions
\item $\lambda: I \rightarrow S$ is an initial weight function
\item $\rho: F \rightarrow S$ is a final weight function.
\end{itemize}
\end{nobreak}
Each transition is element of $Q \times \Sigma \times \Delta \times S \times Q$.
Note, that since $E$ is a multiset, it allows two transitions between states $p$ and $q$ with the same input
and output label, and even the same weight.
However, this is not used in practice.
An example of weighted FST is given in \figref{fst_example}.
\linebreak\linebreak
Finite state transducers have been used widely in many areas, especially linguistics.
They can represent local phenomena encountered in the study of language and they usage often leads to compact representations.
Moreover, it is also very good from the computational point of view, since it advantageous in terms of time and space efficiency.
Whole area of algorithms has been described that consider FSTs.
One of the most important is $determinization$, which determinize paths in the FST and thus allows the time complexity to be linear in the length of input.
Weighted FSTs have been widely used in the area of automatic speech recogniton.
The allows to compactly represent the hypothesis state of the acoustic model as well as combining it with the information contained in the language model.
\img{fst_example.png}{0.9}{Example of Finite State Transducers and their composition}{fst_example}
\section{Automatic speech recognition (ASR)}
\label{ASR-desc}
\subsection*{Overview}
The task in ASR is quite clear: An utterance spoken in natural language should be translated into its text representation.
Formally, we are given a sequence of acoustic observations $\textbf{X} = X_1X_2\dots X_n$ and we want to find out the corresponding word sequence $\textbf{W} = W_1W_2\dots W_m$ that has a maximum posterior probability $P(W | X)$i.e., according to the Bayes rule:
\begin{equation}
\hat{\textbf{W}} = argmax_w \frac{P(\textbf{W})P(\textbf{W}|\textbf{X})}{P(\textbf{X})}
\end{equation}
The problem's solution consists of two subtasks.
First, we have to build accurate acoustic model that describes the conditional distribution $P(W|X)$.
The second problem is to create a language model that reflects the spoken language that should be recognized.
Usually, a variation of the standard $N$-gram approach is sufficient.
\linebreak\linebreak
Individual words are composed of phonetic units, which are in turn modeled using states in probabilistic machinery such as a finite state transducers.
Because the speech signal is continuous, it is transformed to discrete sequence of samples.
MFFC's are commonly used to represent the signal.
The process of deriving this sequence is described in further detail in section \ref{ASP-desc}.
In general, it is difficult to use whole-word models for the acoustic part, because every new task may contain unseen words.
Even if we had sufficient number of examples to cover all the words, the data would be too large.
Thus, we have to select basic units to represent salient acoustic and phonetic information.
These units should be $accurate, trainable$ and $generalizable$.
It turns out that the most appropriate units are phones.
Phones are very good for training and generalization, however, they do not include contextual information.
Because of this, so called triphones are commonly used.
\linebreak\linebreak
To model the acoustics, Hidden Markov Models(HMM) are commonly used, because they can deal with unknown alignments.
In recent years, neural networks have experienced big breakthrough and they found application even in the area of speech recognition.
Namely, recurrent neural networks are able to work with an abstraction of memory and process sequences of variable length, so it overcomes the HMM's in terms of accuracy.
The language and acoustic models are traditionally trained independently and they are joined during the decoding process.
The decoding process of finding the best matched word sequence $\textbf{W}$ to match the input speech signal $\textbf{X}$ in speech recognition systems is more than a simple pattern recognition problem, since there is an infinite number of word patterns to search in continuous speech recognition.
A lattice is built in order to make the decoding.
Its nodes represent acoustic units and edges are assigned costs.
This assignment corresponds to the likelihood determined by the acoustic model as well as the language model, so the FST is a composition of theses two.
Decoding the output is realized as searching the best path through this lattice.
\linebreak\linebreak
The decoding graph is constructed in such a way that the path can contain only words present in the vocabulary the language model is built on.
Thus, each possible path consists of valid words.
Because of that, we can see the word as a basic unit that can't be further split although it is actually composed from phones, or triphones respectively.
However, we can build an FST that allows to construct the path from phones and thus recognize the input on the phonetic level.
Also the search algorithm preserves alternative paths so in the end it gives us not only the best path but also list of alternative hypotheses.
We call it the $n$-best list.
Each hypothesis in the $n$-best list is associated with its likelihood that expresses information from both the language and acoustic model.
In TODO:chapter we explore the $n$-best list and propose method that exploits it.
\label{ASR-phn}
\section{Text-to-speech (TTS)}
\subsection*{Overview}
\cite{taylor2009text}
The text-to-speech problem can be looked at as a task of transforming an arbitrary utterance in natural language from its written form to the spoken one.
In general, these two forms have commonalities in the sense that if we can decode the form from the written signal, then we have virtually all the information we require to generate a spoken signal.
Importantly, it is not necessary to (fully) uncover the meaning of the written signal, i.e. employ Spoken Language Understanding (SLU).
\linebreak\linebreak
On the other hand, we may need to generate prosody which adds some information about emotional state of the speaker or emphasizes certain parts of the sentence and thus changing the meaning slightly.
To obtain prosodic information, sophisticated techniques need to be involved, since its not a part of common written text, except some punctuation.
Another difficulties stem from the fact, that we often need to read numbers, or characters with special meanings such as dates or mathematical equations.
The problem of converting text into speech has been heavily explored in the past and the TTS systems (engines) are very sophisticated nowadays.
The subsequent section, describes briefly the typical architecture of TTS systems.
\subsection{TTS system architecture}
Let us now describe a common use of TTS system.
The architecture usually consists of several modules, although some end-to-end systems base on neural networks have appeared recently (\cite{van2016wavenet}, \cite{wang2017tacotron}).
We give an overview of the typical TTS architecture in \figref{ttsarch}.
First, the input text is divided into sentences and each sentence is further tokenized, based on whitespace characters, punctuation etc.
Then, the non-natural language tokens are decoded and transformed into text form.
We then try to get rid of ambiguity and do prosodic analysis.
Although much of the information is missing from the text, we use algorithms to determine the phrasing, prominence patterns and tuning the intonation of the utterance.
\par
Next is the synthesis phase. The first stage in the synthesis phase is to take the words we have just found and encode them as phonemes.
There are two main approaches how to deal with the synthesis phase.
More traditional approach is so called \textbf{unit concatenation}.
This approach uses a database of short prerecorded segments of speech (roughly 3 segments per phoneme).
These segments are then concatenated together with some use of signal processing so they fit together well.
An alternative is to use machine learning techniques to infer the specification-to-parameter mapping from data.
While this and the concatenative approach can both be described as data-driven, in the concatenative approach we are effectively memorizing the data, whereas in the statistical approach we are attempting to learn general properties of the data.
\par
Trained models are able to transform the phonemes into audio signal representation.
This representation is then synthesized into waveforms using a vocoder module.
Although unit concatenation approaches generally achieve better results, the statistical ones are more flexible and have good possibilities to fine tuning or post processing.
\img{TTSarch.png}{0.9}{Overview of a typical statistical Text-to-speech system architecture. The text is first analyzed and translated to phonetic transcription. Then, the acoustic parameters are estimated. Finally, the speech is synthesized.}{ttsarch}
\subsection{Grapheme to Phoneme conversion and its drawbacks}
\label{g2p-desc}
In the stage of converting graphemes (i.e. text) to phonemes, a $g2p$ module is commonly used.
The g2p task is to derive pronunciations from ortographic transcription.
Traditionally, it was solved using decision trees models.
It can also be formulated as a problem of translating one sequence to another, so neural networks can be used to solve this task (\cite{yao2015sequence}).
However, in our work we use joint-sequence model (\cite{bisani2008joint}), which estimates joint distribution of phonemes based on probabilities derived from $n$-grams.
The $g2p$ module is a crucial component of the system and it has great impact on the final pronunciation.
Since we use machine learning techniques and train on the dictionary data, the model inherently adopts pronunciation rules from the respective language.
Although it is in principle possible to  train multilingual $g2p$ (\cite{schlippe2012grapheme}), it is usually not the case in common TTS systems.
The problem arises when words that originate in some foreign language should be pronounced.
In this work we address this issue by deriving pronunciations from the ASR output.
\subsection*{Speech Synthesis Markup Language (SSML)}
\label{SSML}
\cite{taylor1997ssml}
SSML is an XML standard that allows to specify aspects of the speech.
It is an input to the synthesis module and many TTS engines support its use.
We can specify emotions, breaks etc. with the help of SSML.
More importantly, it can be used to input particular phonemes and thus circumvent the $g2p$ module.
In our work we use this method to feed our phonetic transcriptions into the engine.
The disadvantage of this method is, that many TTS engines process the SSML input imperfectly or not at all.
However, in principle it is possible to add the transcriptions directly into the system's vocabulary.
\begin{center}
\begin{figure}
\textbf{\texttt{
<?xml version="1.0"?>
<speak version="1.0" xmlns="..."
         xmlns:xsi="..."
         xsi:schemaLocation="..."
         xml:lang="en-US">\linebreak
  \tab <voice gender="female">Mary had a little lamb,</voice>\linebreak
  \tab <voice gender="female" variant="2">\linebreak
  \tab\tab Its fleece was white as snow.\linebreak
  \tab</voice>\linebreak
  \tab<voice name="Mike">I want to be like Mike.</voice>\linebreak
</speak>
}}
\label{ssml_example}
\caption{Example of SSML code}
\end{figure}
\end{center}

\subsection*{Overview of the used TTS systems}
\begin{enumerate}
\item Cereproc\footnote{https://www.cereproc.com/}
This engine is a commercial software that is free for educational purposes.
Although it is not open-sourced, we use it for its good quality and reliable outputs.
\item MaryTTS\footnote{http://mary.dfki.de/} MaryTTS is a German open-source project written in Java.
It is highly customizable and modular.
Thus we can explore output of an arbitrary module or replace it in the processing pipeline.
MaryTTS is based on client-server architecture.
\item gTTS\footnote{https://pypi.python.org/pypi/gTTS} is a TTS service provided online by Google.
It achieves very good quality, however it allows ony use of one voice in the free version.
\item Pico \footnote{https://github.com/stevenmirabito/asterisk-picotts} SVOX Pico TTS is a lightweight engine that lacks a good quality of the gTTS, however it offers a large selection of voices and it is commonly used on the phones with Google's Android operating system.

\end{enumerate}
\section{Algorithms description}
\subsection{Dynamic Time Warping (DTW)}\cite{ratanamahatana2004everything}
The measurement of similarity between two time series is an important component of many applications.
Moreover, sometimes we need to align two sequences that describe same data but are of different lengths.
Both this tasks can be solved with use of DTW.
Suppose we have two time series, a sequence $Q$ of length $n$,
and a sequence $C$ of length $m$, where
\begin{center}
$Q = q_1 ,q_2 ,\dots, q_i ,\dots ,q_n$
\linebreak
$C = c_1 ,c_2 ,\dots, c_j \dots, c_m$
\end{center}
To align these two sequences using DTW, we first
construct an n-by-m matrix where the $(i^{th} , j^{th} )$ element of
the matrix corresponds to the squared distance, $d(q_i, c_j) =
(q_i - c_j )^2$ , which is the alignment between points $q_i$ and $c_j$.
To find the best match between these two sequences, we
retrieve a path through the matrix that minimizes the total
cumulative distance between them.
In particular, the optimal path is the path that minimizes the
warping cost:
\begin{equation}
DTW ( Q , C ) = \sqrt[]{\sum_{k=1}^{K}w_k}
\end{equation}
where $w_k$ is the matrix element $(i,j)_k$ that also belongs to $k^{th}$
element of a warping path $W$, a contiguous set of matrix
elements that represent a mapping between $Q$ and $C$.
Methods of dynamic programming are used to fill in the values and find alignment of sequences.
Thus a result of this algorithm is a mapping $\sigma$ that can be used to construct sequence of pairs $A$ containing members of both sequences $C,Q$ in each time step.
$\sigma$ is constructed in such a way, that it holds:
\begin{equation}
A_i = (C_{\sigma (C,i)}, Q_{\sigma (Q, i)}); i = 1 \dots K
\end{equation} 
\begin{equation}
max(|C|,|Q|) \le K
\end{equation}
\begin{equation}
\sum_{i=0}^K d(\sigma (C,i), \sigma(Q,i) is minimal
\end{equation}
Where $d(x,y)$ is an arbitrary distance metric.
In our application, we want to align two sequences of phonemes, hence we can treat it as strings and work with respective distance.
We choose Levenshtein distance to be our metric.
\subsection{Clustering}
Cluster analysis or clustering \footnote{https://en.wikipedia.org/wiki/Cluster\_analysis} is the task of grouping a set of objects in such a way that objects in the same group (called a cluster) are more similar to each other than to those in other clusters.
The measure of similarity can be arbitrary distance measure.
Usually, euclidean distance is used when considering points in space, however it is not a requirement.
Clustering is inherently an unsupervised method, we often want to describe unknown data, that is find some structure or local similarities.
\par
There exist different methods of clustering.
\textit{Hierarchical clustering} is based on the core idea of objects being more related to nearby objects than to objects farther away.
These algorithms connect object" to form clusters based on their distance.
Hierarchical clustering is a bottom-up method, meaning that it forms clusters iteratively.
The hierarchical clustering algorithms' result can be represented as a $dendrogram$, which is a tree diagram representing relations among the explored data.
An example of dendrogram is given in \figref{dendro}.
We can make a horizontal cut to derive respective clusters.
\img{dendrogram.png}{0.9}{An example of dendrogram outputted by hierarchical clustering methods.}{dendro}
\par
On the other hand in \textit{Centroid-based clustering}, clusters are represented by a central vector, which may not necessarily be a member of the data set.
If we fix the number of clusters to $k$, we can see the task as an optimization problem: find the $k$ cluster centers and assign the objects to the nearest cluster center, such that the squared distances from the cluster are minimized.
Typical algorithm from this group is $k$-means clustering\footnote{https://en.wikipedia.org/wiki/K-means\_clustering}. which is an Expectation-Maximization (EM) algorithm.
\par
\textbf{Spectral clustering} \cite{ng2002spectral}
The task of finding good clusters has been the focus of considerable research in machine learning and pattern recognition.
Often, EM algorithms are used to learn a mixture density.
Unfortunately, these approaches have several drawbacks.
First, to use parametric density estimators, simplifying assumptions usually need to be made (e.g., that the density of each cluster is Gaussian).
Second, the log likelihood can have many local minima and therefore multiple restarts are required to find a good solution using iterative algorithms. 
An alternative is to use spectral methods for clustering.
Here, the top eigenvectors of a matrix derived from the distance between points are used.
Therefore, the mentioned problems are solved.
\pagebreak
\subsection{Basic notions in speech communication}
Communication is a process during which some entities exchange information.
There are different types of communication, we focus here on the communication in natural language.
When we consider communication, we should define what we understand by a term form.
This concept deserves some elaboration.
A proper understanding of this part of communication is essential for our purposes, but thankfully we can rely on established frameworks.
\par
The main issue when discussing form, or a message, is to distinguish the general nature of the form from
any specific instantiation it may have.
Imagine, someone wants to deliver an information by showing some color.
If a red card is used, it isn't the card itself that is the important point, rather than the red color was used instead of different color.
So for example, a different red card could be used successfully  as it
is not the particular red card itself which is the form, but rather that the card has the property "red"
and it is this property and its contrasting values that is the basis of the communication system.
We use the term signal to refer to the physical manifestation of the form.
Here the term signal is again used in a specific technical sense: it can be thought of as the thing which is stored or the
thing which is transmitted during communication.
The idea of "red" and the fact that it contrasts with other colors is the form; the physical light waves that travel are the signal.
\par
It is important to see that the relationship between meaning and form, and form and signal
are quite separate.
If we assigned meaning only to colors red, blue and green and an orange card would have been showed to us, we would not decode the meaning, although the signal-form translation worked well.
This situation is similar to that when we read a new word - while are quite capable of using our eyes to read the physical pattern from the page and create an idea of the letters in our head, if we don't know the form-meaning mapping of the word, we can not understand it.
Finally, we use the term channel or medium to refer to the means by which the message
is converted and transmitted as a signal.
\par
In this work, we deal mainly with the spoken channel and the written channel.
Encoding is the process of creating a signal from a message. When dealing with speech, we talk
of speech encoding and when dealing with writing we talk of writing encoding. Speech encoding
by computer is more commonly known as speech synthesis, which is of course the topic of this
book.
The most significant aspect of speech encoding is that the nature of the two representations,
the message and the speech signal, are dramatically different.
A word (e.g. $HELLO$) may be composed of four phonemes ($/H\:EH\:L\:OW/$) but the speech signal is a continuously varying acoustic waveform, with no discrete components.
In this work, we focus on derivation of the first representation and we let the used synthesis frameworks do the speech signal generation.
So we simplify the speech representation to its phonetic transcription.
\par
However, it is important to understand, that we do not change the meaning or even form of the communication, we just want to use different channel for the transmition.
When we consider the verbal language, we may talk about the duality of form such that we have phonemes which combine to words which combine to form sentences.
We define three terms that help us to describe the problematics:
\begin{enumerate}
\item \textbf{phonemes} are members of the relatively small set of units which can be combined to produce distinct word forms.
\item Term \textbf{phone} is used to describe a single speech sound, spoken with a single articulatory configuration.
\item \textbf{allophones} link the phonemes and phones: different ways of realizing a single phoneme are called allophones.
\end{enumerate}
The duality principle states that verbal language is not one system but two; the first system uses a
small inventory of forms, phonemes, which sequentially combine to form a much larger inventory
of words.
\par
We have a similar dual structure in written communication, where words are made of \textbf{graphemes}.
Graphemes are in many ways analogous to phonemes, but differ in that they vary much more from language to language.
In alphabetic languages like English, graphemes can be thought of as letters or characters; in syllabic writing like Japanese hiragana, each grapheme represents a syllable or mora (unit that determines syllable weight, i.e. stress or accent) and in languages like Chinese each grapheme represents a full word or part of a word.
We consider alphabetic languages in this work, encoded with the Latin alphabet\footnote{https://en.wikipedia.org/wiki/Latin\_alphabet}.
However, it is possible to extend the used grapheme-to-phoneme conversion technique \cite{bisani2008joint} to work with other types of languages.
\par
To encode the phonemes, phonetic alphabets are used.
The standard is an International Phonetic Alphabet (IPA)\footnote{https://en.wikipedia.org/wiki/International\_Phonetic\_Alphabet}.
IPA symbols are composed of one or more elements of two basic types, letters and diacritics.
The general principle of the IPA is to provide one letter for each distinctive sound (speech segment), although this practice is not followed if the sound itself is complex.
An example of IPA characters is given in \figref{ipa_overview}.
\img{ipa_overview.png}{0.9}{Some letter of the IPA alphabet together with their occurrences in English words.}{ipa_overview}
\par
The problem is that some of the tools we work with use different phonetic alphabets with possibly different rules.
However, the TTS engines work with IPA, so it is necessary to convert the transcriptions.
To overcome this problem, we have derived one-to-one mappings between these sets and IPA.
This process may introduce some errors, however the alphabets are usually based on similar principles, so it is no so hard to derive this mappings.
Namely, we used the Arpabet\footnote{https://en.wikipedia.org/wiki/Arpabet}, Timit\footnote{https://catalog.ldc.upenn.edu/docs/LDC93S1/PHONCODE.TXT} and CGN\footnote{https://pdfs.semanticscholar.org/64d5/d612f9c639271a1340820a418fa5baf02770.pdf} phoneme sets.
\par
As we have suggested, some speech synthesizers are able to accept input in the SSML format which we introduce in \ref{SSML}.
However, not all the synthesizers have this ability.
We use Cereproc to work with utterances written in SSML.
An example of such utterance is given in \figref{SSML_trn}.
The problem with providing transcriptions in this way is, that the Cereproc synthesizer ignores the characters defining stretch of the phoneme.
Therefore, we omit these characters from the transcriptions by mapping the stretched phonemes to their base variants
This leads to worse quality of the pronunciation, however is sufficient to explore if the transcription contains sensible phonemes.
\img{ssml_ipa.png}{1.0}{An example of the synthesizer's input in SSML, forcing it to use our phonetic transcription.}{SSML_trn}
\pagebreak
\section{Related Work}
\label{relatedwork}
\subsection{Grapheme-to-phoneme conversion}
An automatic grapheme-to-phoneme conversion was first considered in the context of TTS applications. The input text needs to be converted to a sequence of phonemes which is then fed into a speech synthesizer.
It is common in TTS systems that they first try to find the desired word in the dictionary and if it doesn't find it, it employs the grapheme-to-phoneme ($g2p$) module.
A trivial approach is to employ a \textit{dictionary look-up}.
However, it cannot handle context and inherently covers only finite set of combinations.
To overcome this limitations, the rule-based conversion was developed.
Kaplan and Kay \cite{kaplan1994regular} formulate these rules in terms of finite-state automata.
This system allows to greatly improve coverage.
However the process of designing sufficient set of rules is difficult, mainly since it must capture irregularities.
Because of this, a \textit{data-driven} approach based on machine learning has to be employed.
Many such techniques were explored, starting with Sejnowski and Rosenberg \cite{sejnowski1988nettalk}.
The approaches can be divided into three groups.
\subsubsection{Techniques based on local similarities}
The techniques presuppose an alignment in the training data between letters and phonemes or create such an alignment in a separate preprocessing step.
The alignment is typically construed so that each alignment item comprises exactly one letter.
Each slot is then classified (using its context) and a correct phoneme is predicted.
Neural networks and decision tree classifiers are commonly used for this task.
\subsubsection{Pronunciation by analogy}
This term is typically used for methods that could be described as nearest-neighbor-like.
They search for local similarities in the training lexicon and the output pronunciation is chosen to be analogous to retrieved examples. 
In the work of Dedina and Nusbaum \cite{dedina1991pronounce} the authors first identify words from the database that match the input string and then build a pronunciation lattice from them.
Paths through the lattice then represent the derived pronunciations.
\subsubsection{Probabilistic approaches}
The problem can also be viewed from a probabilistic perspective.
Pioneers in this area were Lucassen and Mercer \cite{lucassen1984information}.
They create $1-to-n$ alignments of the training data using a context independent channel model.
The prediction of the next phoneme is based on a symmetric window of letters and left-sided window of phonemes.
Authors then construct regression tree, the leafs of which carry probability distribution over phonemes.
\linebreak\linebreak
\label{g2p-jseq}
A popular approach based on probability modeling is to employ so called \textit{joint sequence models} \cite{bisani2008joint}.
We use an open-source implementation of such a model in our work.
This approach formalizes the task as follows:
\begin{equation}
\boldsymbol{\varphi(g)} = \argmax_{\varphi'\in\Phi^*} p(\boldsymbol{g,\varphi'})
\end{equation}
where * denotes a Kleene star.
In other words, for a given ortographic form $\boldsymbol{g} \in G^*$ we want to find the most likely pronunciation $\boldsymbol{\varphi} \in \Phi^*$, where $G, \Phi$ are ortographic and phonetic alphabets.
The fundamental idea of joint-sequence models is that the relation of input and output sequences can be generated from a common sequence of joint units.
These units carry both input and output symbols.
Formally, the unit called \textit{grapheme} is a pair $q = (\mathbf{g}, \mathbf{\varphi}) \in Q \subset G* \times \Phi$ where $\mathbf{g}$ and $\mathbf{\varphi}$ are letter and phoneme sequences which can be of different lengths.
In the simplest case, each unit carries zero or one input and zero or one output symbol.
This corresponds to the conventional definition of finite state transducers (FST) that are described in \ref{fst-desc}.
The letter and the phoneme sequences are grouped into an equal number of segments.
Such a grouping is called a joint segmentation.
The joint probability is defined as:
\begin{equation}
p(\mathbf{g}, \mathbf{\varphi}) = \sum_{\mathbf{q}\in S(\mathbf{g},\mathbf{\varphi})}p(\mathbf{q})
\end{equation}
since there are many possible groupings of input sequences in general.
The joint probability distribution $p(\mathbf{g}, \mathbf{\varphi})$ has thus been reduced to a probability distribution $p(\mathbf{q})$ over graphone sequences $\mathbf{q} = q_1, \dots, q_K,$ which can be modeled using  $N$-gram approximation:
\begin{equation}
p(q_1^K) \cong \prod_{j=1}^{K+1} p(q_j\vert q_{j-1},\dots,q_{j-M+1})
\end{equation}
The probability distribution is then typically estimated by an Expectation-Maximization algorithm.
\linebreak \linebreak
Generally, the problem of the wrong pronunciation in TTS  is caused by a bad phonetic transcription.
Traditional TTS systems are modular, one module's output is inputted into the next one.
Because of this fact, the errors cumulate and thus the mistakes made by $g2p$ cannot be repaired.
So if we want to improve the pronunciation, we can try to improve the $g2p$ as it is done in \cite{deri2016grapheme}.
Authors in this work propose a method of exploiting a $g2p$ trained on a language with a high number of available resources to create a $g2p$ for language for which we do not have sufficient number of examples.
This method relies on the existence of a conversion mapping between these two languages.
Also it requires to do the conversion for every new language.
In theory, a model can be created, that is able to transcribe grapheme sequences into an appropriate phonetic representation and can handle multiple languages.
However, it needs to somehow obtain information, which language the input sequence comes from, which is not straightforwardly doable.
This method has several drawbacks, because the language is not always known and the set of known languages is limited, so it does not really solve the OOV problem.
Also, it potentially requires a lot of training data.
Moreover, if we want to learn a new pronunciation of just one word, it's more convenient to do it in a different way.
\subsection{Learning pronunciation from spoken examples}.
\label{pronunc-spoken}
This group of methodologies aim to derive phonetic transcriptions directly from audio input.
They are built on the theory of Automatic Speech Recognition which we discuss in \ref{ASR-desc}.
Authors of \cite{slobada1996dictionary} introduce method of deriving correct pronunciation for a word in order to enlarge the recognizer's dictionary.
They propose a data-driven approach to automatically add new words and their variants, respectively.
The authors argue that in the spontaneous speech, the most frequent pronunciation does not need to be the one that is marked as correct and is used in the training phase.
Thus the overall performance of the recognizer may be degraded since the phonetic units are bound with inadequate acoustics.
The method is proposed, that relies on the use of both phoneme and word-level speech recognizer.
The phoneme recognizer is constructed using smoothed bigram Language model.
We discuss the phoneme recognizers in more detail in \ref{ASR-phn}.
The algorithm collects all occurrences of words in the database and creates phonetic transcriptions using the recognizer.
It then sorts the variants, rejects some of them an creates an $n-best$ list which is added to the dictionary.
The recognizer can then be retrained, allowing multiple pronunciations for each word.
Thus the recognition performance can be improved by an automatic procedure without the need for using the phonological rules.
\linebreak\linebreak
In the work of McGraw et al. \cite{mcgraw2013learning}, the concept of Pronunciation Mixture Models is introduced.
The authors use a special kind of speech recognizer, the search space of which has four primary hierarchical components: a language model $G$, a phoneme lexicon $L$, phonological rules $P$ that expand the phoneme pronunciations to their phone variations, and a mapping from phone sequences to context-dependent model labels
$C$.
All of the components mentioned can be with advantage represented as FSTs and thus the full decoder network can be represented as a composition of these components: $R = C\circ P \circ L \circ G$.
A probabilistic lexicon is considered, meaning, that several pronunciations are allowed for each word and there is no hard limitation that would force the recognizer to choose one.
Instead, a kind of soft voting is considered, meaning, that each transcription is used with certain probability.
Also, joint-sequence modeling is considered, as we introduced in \ref{g2p-jseq}, so each transcription is considered together with its orotgraphic form.
That means, that we can describe the log-likelihood of $M$ utterances $ D = \{\mathbf{u}_i,\mathbf{W}_i\}$ where $\mathbf{u}_i$ are speech data and $\mathbf{W}_i$ their transcriptions as follows:
\begin{equation}
\mathcal{L}(\Theta\vert D) = \sum_{i=1}^M log \sum_{B \in \mathcal{B}} P(\mathbf{u}_i,\mathbf{B},\mathbf{W}_i;\Theta)
\end{equation}
where $\mathbf{B}$ are respective phone sequences (i.e. pronunciations) and $\Theta$ represents the model parameters.
Then, we derive using a chain rule:
\begin{equation}
P(\mathbf{u}_i,\mathbf{B},\mathbf{W}_i;\Theta) =  P(\mathbf{u}_i\vert\mathbf{B})P(\mathbf{B}\vert\mathbf{W}_i;\Theta)P(\mathbf{W}_i)
\end{equation}
If we further assume, that pronunciation sub-units $\mathbf{b}_i \in \mathbf{B}$ are context independent, we can transcribe the above expression:
\begin{equation}
P(\mathbf{u}_i\vert\mathbf{B})P(\mathbf{B}\vert\mathbf{W}_i;\Theta)P(\mathbf{W}_i) = P(\mathbf{u}_i\vert\mathbf{B})(\prod_{j=1}^{k_i}P(\mathbf{b}_j\vert \mathbf{w}^i_j;\Theta))P(\mathbf{W}_i) 
\end{equation}
The model parameters are then estimated using the EM-algorithm.
Parameters related to the language model, can be initialized with use of graphone language model.
Several technical issues has to be dealt with, however the Pronunciation Mixture Models can be trained on the same data as traditional ASR engines and can be used to obtain phonetic transcriptions from audio data.
\linebreak\linebreak

Similar approach is considered in the work of Reddy and Gouvea \cite{reddy2011learning}, except they do not have access to acoustic models or phone lattices.
They use only the mistakes done by the recognizer as their source of information.
An OOV word is passed through an ASR decoder giving an $n$-best word recognition output.
Since the words are OOVs, every hypothesis will be a recognition mistake.
These mistakes are then exploited, assuming that the following generative story of the recognition output
for a word $\mathbf{w}$ holds:
\begin{enumerate}
\item A pronunciation baseform $\mathbf{b}$ is drawn from the distribution $\Theta$.
\item A phonetic confusion function from the word $\mathbf{w}$ and the selected baseform $\mathbf{b}$ is applied in order to generate a phoneme sequence $\mathbf{p}$ with probability $P(\mathbf{p}|\mathbf{b},\mathbf{w})$
\item A word sequence $\mathbf{e}$ with probability $P (\mathbf{e} \vert \mathbf{p},\mathbf{b},\mathbf{w}) = 1$ 
is generated using the pronunciation lexicon.
\end{enumerate}
The authors model the joint probability of hypothesis and reference word $P(\mathbf{e}, \mathbf{w}) = P(\mathbf{w}\sum_b f_{e,b,w})$, where $f_{e,b,w} = P(\mathbf{e}\vert \mathbf{b}, \mathbf{w})$ is the phonetic confusion function and it is used to estimate the distribution $P(\mathbf{b}\vert\mathbf{w},\mathbf{e})$.
Thus they are able to derive pronunciations without access to ASR lattices, i.e. it only consideres the recognizer as a black-box. 
\subsection{Conclusion}
Many approaches were introduced that are able to convert an utterance in ortographic or audio form to its phonetic representation.
$G2P$ converts the grapheme transcriptions, using only the text input.
It is a well explored field of study with many different variants of realization.
Although it achieves very good results nowadays, it suffers from the fact that same groups of letters may have different pronunciations in different languages.
We typically don't have access to the information which language is considered and it may be difficult to get access to sufficient number of datasets.
The latter problem can be partially solved by transfer learning as proposed in \cite{deri2016grapheme}.
\linebreak\linebreak
Alternatively, one can derive pronunciations directly from an audio signal.
This approach has been also explored by some authors, however it usually requires quite low-level modifications of the speech recognizer.
Also, the authors used the derived pronunciations to enlarge the phonetic dictionary of the recognizer, not to improve the Text-To-Speech systems.
We explore methods of merging the mentioned approaches to combine both textual and acoustic information and its usability when confronted with human judgments.